\section{Experimentaci\'on}

Para evaluar el impacto del delay y las perdidas de paquetes dise\~namos un set de experimentos para estudiar la variaci\'on del RTT ante los cambios de estas variables. 

Para correr los experimentos creamos un cliente y un server que intercambian paquetes. Una vez establecida la conexi\'on, el server le avisa al cliente que ya puede mandar los datos, y ah\'i es cuando el server empieza a medir el tiempo hasta que recibe el \'ultimo byte del cliente. 

A partir de este modelo, hay que tener en cuenta que el tama\~no del buffer del server va a influir en el RTT final, ya que cu\'anto m\'as chico, m\'as r\'apido se llena y el cliente tiene que esperar m\'as veces para mandar la misma cantidad de paquetes. Para ver m\'as claro esto, abajo est\'a el c\'odigo del server que levanta los datos y toma el tiempo de lo que tarda en mandarse desde el cliente: 

\begin{verbatim}
    startTime = time.time()
    str_recv = ""
    while (size_to_recv > len(str_recv)):
      str_recv += sock1.recv( MAX_BUFFER )
    totalTime = time.time() - startTime
\end{verbatim}

Asique el cliente le llena el b\'uffer al server, y luego debe esperar a que \'este lo vac\'ie y le conteste con el ACK correspondiente, y as\'i seguir hasta que se mand\'e toda la data. 

Los siguientes experimentos los corrimos con un cliente y un server que est\'an en el mismo host, para que las mediciones no est\'en viciadas por factores de una LAN real.

\subsection{Bytes enviados vs Tiempo}
\subsubsection{Experimento A (Impacto de la p\'erdida)}

  Para evaluar el impacto de la p\'erdida de paquetes, creamos una conexi\'on que no tuviera delay y obtuvimos los siguientes gr\'aficos: 
  
  \ponerGrafico{./graficos/size_vs_tiempo_b512_n36_[perdida].png}{Buffer de 512 Bytes}{0.4}{size_tiempo_perdida_512}

  \ponerGrafico{./graficos/size_vs_tiempo_b1024_n49_[perdida].png}{Buffer de 1024 Bytes}{0.4}{size_tiempo_perdida_1024}

  En ambos gr\'aficos vemos como en cada m\'ultiplo del buffer las curvas tienen un salto, pero sin embargo parece que los distintos porcentajes de p\'erdidas no influyen en el tiempo, ya que los RTT son similares. 
  
\subsubsection{Experimento B (Impacto del delay)} 

  \ponerGrafico{./graficos/size_vs_tiempo_b512_n36_[delay].png}{Buffer de 512 Bytes}{0.4}{size_tiempo_delay_512}
  
  \ponerGrafico{./graficos/size_vs_tiempo_b1024_n49_[delay].png}{Buffer de 1024 Bytes}{0.4}{size_tiempo_delay_1024}
  
  En este caso, otra vez se mantiene los saltos de tiempos cuando la cantidad de bytes supera un m\'ultiplo del buffer. Pero a diferencia del Experimento A, el cambio de delay s\'i afecta al RTT. 

\subsection{Delay vs Tiempo}
\subsubsection{Experimento A}

  El siguiente gr\'afico lo hicimos usando una conexi\'on sin p\'erdida de paquetes y cuyo server usaba un B\'uffer de 1024 bytes.

  \ponerGrafico{./graficos/delay_vs_tiempo.png}{}{0.4}{delay_tiempo}

  Paralelamente a los gr\'aficos \ref{fig:size_tiempo_delay_512} y \ref{fig:size_tiempo_delay_1024}, el tiempo crece en funci\'on del delay, y las curvas de los datos que usan las mismas r\'afagas de 1024 bytes para llegar del cliente al server tienen el mismo delay. En este caso, 500 bytes y 1000 bytes por un lado, por otro 1500 y 2000 Bytes, y por \'ultimo 2500 B. 

\subsection{P\'erdida vs Tiempo}
\subsubsection{Experimento A}

  En este experimento la conexi\'on tiene delay nulo, y el server tiene un b\'uffer de 1024 bytes. 
  
  \ponerGrafico{./graficos/perdida_vs_tiempo.png}{}{0.4}{perdida_tiempo}
  
  Como hab\'iamos dicho en los experimentos de \textit{Bytes enviados vs Tiempo} y a diferencia del delay, el porcentaje de perdida parece no afectar el tiempo que se tarda en enviar $n$ byte. Sin embargo, para los datos enviados que usan la misma cantidad de r\'afagas de 1024 bytes las curvas son similares. 

\subsection{Bytes enviados vs Throughput}
  Por \'ultimo para ver el comportamiento del throughput en funci\'on del tama\~no enviado y del delay, establecimos una conexi\'on con p\'erdida de paquetes nula entre dos hosts, donde el server ten\'ia el buffer en 1024 bytes. 

  \ponerGrafico{./graficos/size_vs_throughput.png}{}{0.4}{size_throughput}
  
  Como era de esperar, a mayor delay menor es el throughput. Tambi\'en podemos observar, curiosamente, que hay una mayor eficiencia del throughput cuando se aprovecha todo el b\'uffer del server. 
  
\subsection{Delay vs Throughput}
  
  Para ver m\'as claramente como influye el delay directamente en el throughput tomamos el experimento anterior y modificamos las variables del gr\'afico, quedandonos de la siguiente manera: 
  
  \ponerGrafico{./graficos/delay_vs_throughput.png}{}{0.4}{delay_throughput}
  
  Consecuentenmente con los gr\'aficos anteriores, comprobamos que el throughput disminuye muy r\'apidamente ante un poco de delay. 
  
\subsection{Consideraciones}
  
  Cuando vimos que no conseguimos ning\'un cambio en los resultados ante la variaci\'on del porcentaje de p\'erdida de los paquetes, pensamos que tal vez hubiera un error de implementaci\'on en \code{se\_perdio\_paquete()}, o que debi\'eramos reubicarlo. 
  
  Para comprobar que no estuviera fallando la ubicamos en la primitiva \code{send()} de la clase \code{Soquete}, que es la que terminan llamando todos los m\'etodos que env\'ian datos. Fue en este caso que s\'i se notamos cambios en los casos de tests y en la performance de la conexi\'on. Con probabilidad $<100\%$ de p\'erdida de mensajes, algunos se mandaban sin problemas, otros eran necesario ser reenviados. Sin embargo, en distintos momentos aparentemente aleatorios (o quiz\'a en ning\'un momento de todo el test), la conexi\'on se tildaba y tanto el cliente como el servidor dejaban de intercambiar mensajes. 
  
  Para debugear un poco mas, fuimos reubicando \code{se\_perdio\_paquete()} en distintos m\'etodos que mandan mensajes, y nos encontramos con que si el paquete que se pierde es el de Update Window que env\'ia el server, ah\'i es cuando la conexi\'on se cuelga. Por lo tanto, no pudimos reubicar la funci\'on \code{se\_perdio\_paquete()} a un lugar que nos permitiera testear sin que en alg\'un momento se colgara, o sin conseguir alg\'un cambio significativo. 
  
  Por otro lado, con la implementaci\'on de que solo se perdieran los ACK de los datos, como hab\'ia propuesto la c\'atedra, tambi\'en nos pregunt\'abamos por qu\'e suced\'ia que cuando la p\'erdida era total la conexi\'on segu\'ia funcionando y el cliente y el servidor se comunicaban sin ninguna traba. 
  
  La respuesta a esto tambi\'en la encontramos en el Update Window enviado por el servidor, porque si bien los ACK al cliente que manda el primero se pierden, cuando informa del tama\~no de su ventana manda el paquete con el ACK que el cliente estaba esperando. 